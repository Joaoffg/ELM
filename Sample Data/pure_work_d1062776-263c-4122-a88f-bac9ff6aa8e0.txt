1032310
research-article2021
NMS0010.1177/14614448211032310new media & societyGonçalves et al.
Article
Common sense or censorship:\nHow algorithmic moderators\nand message type influence\nperceptions of online content\ndeletion
new media & society\n﻿1­–23\n© The Author(s) 2021\nArticle reuse guidelines:\nsagepub.com/journals-permissions
https://doi.org/10.1177/14614448211032310\nDOI: 10.1177/14614448211032310
journals.sagepub.com/home/nms
João Gonçalves
Erasmus University, The Netherlands
Ina Weber
Erasmus University, The Netherlands; University of Antwerp, Belgium
Gina M. Masullo
University of Texas at Austin, USA
Marisa Torres da Silva
Universidade Nova de Lisboa, Portugal
Joep Hofhuis
Erasmus University, The Netherlands
Abstract\nHateful content online is a concern for social media platforms, policymakers, and\nthe public. This has led high-profile content platforms, such as Facebook, to adopt\nalgorithmic content-moderation systems; however, the impact of algorithmic\nmoderation on user perceptions is unclear. We experimentally test the extent to which\nthe type of content being removed (profanity vs hate speech) and the explanation given\nfor its removal (no explanation vs link to community guidelines vs specific explanation)\nCorresponding author:\nJoão Gonçalves, Department of Media & Communication, Erasmus University, P.O. Box 1738, 3000 DR\nRotterdam, The Netherlands.\nEmail: ferreiragoncalves@eshcc.eur.nl
2
new media & society 00(0)
influence user perceptions of human and algorithmic moderators. Our preregistered\nstudy encompasses representative samples (N = 2870) from the United States, the\nNetherlands, and Portugal. Contrary to expectations, our findings suggest that\nalgorithmic moderation is perceived as more transparent than human, especially when\nno explanation is given for content removal. In addition, sending users to community\nguidelines for further information on content deletion has negative effects on outcome\nfairness and trust.\nKeywords\nArtificial intelligence, content moderation, cross-country, experiment, hate speech,\nprofanity, social media
In May 2020, Twitter censored a post by US President Donald Trump related to the\nGeorge Floyd protests with the argument that it was glorifying violence. In contrast,\nFacebook opted to keep the post unlabeled, which in turn generated backlash against the\nplatform (Paul, 2020). User support and attitudes toward content moderation are key\naspects that platforms should consider when designing their content moderation policies\n(Riedl et al., 2021). While some studies have looked at the impacts of moderation practices on users who have had their content flagged or removed (e.g. Jhaver et al., 2019a;\nMyers West, 2018), there is little information on how these practices affect perceptions\nof the large portion of users who have never engaged with moderation teams,1 but are,\nnonetheless, important stakeholders for platforms.\nPerceptions of the types of content that should be allowed on social media are particularly relevant when considering harmful content like hate speech and novel content moderation practices like machine learning classification. Debates over migrants, the\nreemergence of far-right extremism in Europe and in the United States, and online cultures of misogyny, xenophobia, and racism have highlighted the growing problem of\nboth online hate speech (Pohjonen, 2019) and less virulent but still antagonistic content\nsuch as profanity (Chen, 2017; Krämer and Springer, 2020). In Q1 2019, Facebook\nremoved about 4 million pieces of content due to hate speech, numbers that make it clear\nthat human moderators are inadequate alone to handle this amount of content.\nConsiderable progress has been achieved in developing automated ways of detecting\nhateful content (e.g. Djuric et al., 2015; Zhang et al., 2018) and profanity (Gillespie,\n2018). However, qualitative research (Myers West, 2018; Suzor et al., 2019) shows that\neven if moderation algorithms are developed, their implementation may be compromised\nby the way content removal decisions are communicated. For instance, a study of\nWikipedia (Halfaker et al., 2012) concludes that algorithmic moderation tools are one of\nthe likely causes for a reduction in participation on the platform, and research on Reddit\nshows that moderators fall short when implementing transparency and accountability\nprinciples (Juneja et al., 2020). Besides academic studies, scrutiny of algorithmic decision-making has also entered the public agenda through documentaries such as Coded\nBias (Kantayya, 2020). We study content moderation from the perspective of a bystander,
Gonçalves et al.
3
because the technical progress made toward automated content moderation may be\nmeaningless if the user community questions the justice and fairness of moderation.\nIncreased perceptions of justice and fairness have been shown to reduce future norm\nviolations on social media platforms (Tyler et al., 2021), while messages from the platforms themselves can increase these perceptions. Even if users are not involved in moderation processes directly, perceiving the moderation of other users’ content as fair\nshould still reduce deviant behavior through social learning (Bandura, 2005) and may\nincrease bystander intervention to enforce norms (Naab et al., 2016).\nUsers want content moderation (da Silva, 2015), but they may also perceive it negatively (Myers West, 2018; Suzor et al., 2019). We argue that it is essential to understand\nbetter what factors play a role in user support for different types of moderation. Our\nstudy examines the interplay of three factors that have not yet been studied together:\nmoderator type (human vs AI [artificial intelligence]), explanation for content removal\n(no reason for removal vs presenting general community guidelines vs giving specific\nremoval reason), and type of content (hate speech vs profanity). We examine the influence of these factors on five outcomes related to organizational justice. For a stringent\ntest, we conduct preregistered (https://osf.io/2n7d5/?view_only=d029d9ffa052481f83ee\nb9e67717a8da) online between-subjects experiments in three countries with a representative sample (N = 2870).\nThe findings from this study contribute new knowledge on algorithmic aversion\n(Dietvorst et al., 2015) and hate speech while also offering practical recommendations\nfor online content managers. Furthermore, the comparison of European (Netherlands and\nPortugal) and North American (United States) samples allows us to present findings\nbeyond a single cultural and legal context, although generalizations are still conditioned\nby the fact that we are looking at a specific type of moderation, one that is managed by\nthe platforms themselves and situated in specifically national and historical contexts.
Assessing content moderation\nDespite early optimism that the Internet could be a new public sphere (Papacharissi,\n2002), it soon became apparent that online spaces often devolved into wastelands of\ntoxicity unless content was moderated (Goodman and Cherubini, 2013). While content\nmoderation is widespread, many questions surround its implementation. Although websites publish community guidelines to clarify moderation processes, users are still often\nfrustrated with the process and outcome of content moderation (Myers West, 2018).\nFurthermore, when AI is used, accountability mechanisms are less clear, and algorithms\nare disproportionally penalized for mistakes in user perceptions when compared to\nhumans (Dietvorst et al., 2015).\nWhile many studies on algorithmic aversion often frame the problem as a matter of\nuser preferences or perceptions of accuracy (see Castelo et al., 2019), content moderation\ninvolves different facets that cannot be subsumed under a single construct. In the face of\nthis complexity, the concept of organizational justice is an adequate starting point because\nit describes how people perceive the fairness of decisions that involve them (Colquitt,\n2001; Lee, 2018). Moderators and users always interact in the framework of an organization, be it a social media website like Facebook or a news outlet. Organizational justice
4
new media & society 00(0)
is useful to tackle the multiple dimensions of content moderation because it encompasses\ndistributive justice, procedural justice, and interactional justice (Colquitt et al., 2005).\nRather than focusing on a single form of organizational justice, we position this work in\nthe integrative wave of organizational justice research (Colquitt et al., 2005: 35), assessing five indicators that relate the different forms of organizational justice: outcome fairness, procedural fairness, transparency, legitimacy, and trust. These concepts capture\nhow justice is perceived by bystanders within the organization, specifically perceptions\nof how others’ content is moderated. While some of the concepts have been shown to be\ntheoretically and empirically related, they, nevertheless, correspond to different dimensions of content moderation. This allows us to explore nuances that might have been\nmissed by studies that examined only one or two constructs or a single understanding of\norganizational justice.\nTraditionally, the idea of justice branches into procedural fairness and outcome fairness. Outcome fairness concerns perceptions regarding how just the results, consequences, and their distributions are. In contrast, a process may be perceived as fair if it\nincludes the affected people in the process, upholds ethical and moral standards, ensures\nimpartiality and consistency of decision-making, and provides the possibility of correcting the process (Colquitt, 2001; Ötting and Maier, 2018). While a fair process increases\nthe likelihood that the outcome will also be perceived as fair, these are distinct concepts\nand dimensions.\nOther concepts to consider for moderation are legitimacy and trust, because they pertain to whether people perceive the moderator as having the authority to delete content.\nLegitimacy is defined as the acceptance of an entity as having the right to make decisions\nthat impact oneself and feeling obligated to follow these decisions (van Dijke et al.,\n2010). A legitimate moderator is perceived as having the right to make decisions about\nuser content, and users accept the moderator’s decisions (Tyler, 2006; van der Toorn\net al., 2011). Trust is the conviction that an authority possesses integrity, honesty, and\nbenevolence that render it trustworthy, so it is not perceived as exploitive (Rawlins,\n2008).\nFinally, in line with the fairness, accountability, transparency (FAT) framework for\ncontent moderation, we consider perceptions of transparency (Jhaver et al., 2019c).\nTransparency requires that an organization provide information about its actions that are\n“truthful, substantial and useful,” so that the organization can be held accountable\n(Rawlins, 2008: 74). Suzor et al. (2019) emphasize the need for meaningful transparency\nfor content moderation, such as providing sufficient explanations for why content was\nremoved to strengthen users’ trust in moderation and to ensure accountability of moderators and platforms.
Hate speech and profanity\nOf aversive content posted online, hate speech has been of particular concern to platforms (Facebook, 2020), policymakers (European Commission, 2020), and scholars\n(Brown, 2017; Erjavec and Kovačič, 2012) because of its frequency online and negative\nconsequences (Soral et al., 2018). Defining hate speech poses challenges because of differences in social and legal contexts.
Gonçalves et al.
5
The European Commission against Racism and Intolerance (ECRI; 2016), for example, defines hate speech as\nthe use of one or more particular forms of expression—namely, the advocacy, promotion or\nincitement of the denigration, hatred or vilification of a person or group of persons, as well any\nharassment, insult, negative stereotyping, stigmatization or threat of such person or persons and\nany justification of all these forms of expression. (p. 16)
The definition excludes expressions that merely distress, hurt, or offend because hate\nspeech is much more than mere dislike or bias and it tends to be discriminatory, abusive,\nand hostile in nature (Richardson-Self, 2018; SELMA, 2019). In contrast, US law does\nnot outlaw hate speech, and courts have repeatedly ruled that it is constitutionally protected speech.\nHowever, the US Constitution does not prohibit private businesses from limiting speech.\nOnline platforms can enforce their own definitions of hate speech when deciding whether\nto remove content (Gagliardone, 2019). Facebook, for instance, defines it as “a direct\nattack on people based on what we call protected characteristics” (Facebook, 2020: n.p.).\nFaced with the challenge of defining hate speech in our study in different national contexts, we focus on the two characteristics that are more consistent across contexts: (1) hate\nspeech targets vulnerable or protected groups, and (2) hate speech implies serious and\ndiscriminative expressions against that group. It is noteworthy that the concept of protected\ngroups exists in the United States, even though there is no legal concept of hate speech.\nWhile scholars, platforms, and policymakers see hate speech as a particularly problematic form of online content, it is unclear whether users share this understanding and\ndistinguish it from other forms of antagonistic content, such as profanity. Profanity is\nmore frequent online than hate speech (Chen, 2017; Coe et al., 2014) and people readily\nidentify it as aversive (Kenski et al., 2017; Muddiman and Stroud, 2017). Yet, profanity\nimplies poor manners rather than threats to democracy and its basic principles\n(Papacharissi, 2004). Because hate speech is universally considered more harmful than\nprofanity, it seems plausible that people would see efforts to moderate hate speech as\nfairer than efforts to moderate profanity. However, it is also possible that users would\nconsider efforts to moderate profanity as fairer, because profanity is immediately noticeable as aversive (Kenski et al., 2017) while hate speech is open to interpretation based on\ncontext (Roussos and Dovidio, 2018). Due to the lack of a clear argument for directionality and the importance of context, H1 is put forward as a non-directional hypothesis:\nH1. There are differences in perceived procedural fairness (H1a) and outcome fairness (H1b) between the removal of hate speech and the removal of profanity\ncontent.
Algorithmic aversion\nWhile machine-learning approaches outperform human judgment in many situations,\nthere is still an implicit tendency for individuals to prefer human decisions or advice.
6
new media & society 00(0)
This preference has been labeled as algorithm aversion (Dietvorst et al., 2015), and it has\nbeen verified in different domains, such as medicine (Longoni et al., 2019) and employee\nrecruitment (Diab et al., 2011), among others. Despite evidence for algorithm aversion\n(see Burton et al., 2020, for a review), some studies show that people prefer algorithmic\njudgments (Logg et al., 2019) when the focus is on decision-making and assessing outcomes. In particular, recent work on perceptions of AI focuses on fairness as the key\nmetric to assess attitudes toward decision-makers (Helberger et al., 2020):\nH2. There are differences in perceived procedural fairness (H2a) and outcome fairness (H2b) of content removal between human and algorithmic moderators.\nDespite diverse findings regarding the assessment of algorithmic and human judgments, algorithms are consistently seen as underperforming when compared to humans\nin certain tasks. People penalize algorithms disproportionally for making mistakes\n(Dietvorst et al., 2015). As a relatively new mechanism in content moderation, AI moderators, therefore, may lack the legitimacy and trust that people assign to their more\nfamiliar human counterparts. Furthermore, because algorithms operate behind the scenes,\ntheir black box nature makes perceptions of transparency particularly important (Shin\nand Park, 2019).\nH3. Algorithmic moderators are perceived as less trustworthy (H3a), transparent\n(H3b), and legitimate (H3c) than human moderators.\nOne key idea that underlies algorithm appreciation or aversion is task dependency,\nwhich refers to the characteristics of a given task and, in particular, the degree to which\na task is seen as requiring subjective or objective reasoning (Castelo et al., 2019; Lee,\n2018). While content moderation at first may appear as a single task, different types of\ncontent require moderators to employ distinct interpretative frameworks. For example,\nbecause profanity and hate speech are conceptually different, moderation decisions may\ndraw on different abilities and resources. Because characteristics of profanity are explicit,\nliteral, and independent of context (Kenski et al., 2017; Muddiman and Stroud, 2017),\nprofanity can unambiguously be classified as such and filtered. In contrast, hate speech\ncan also be implicit but does not have to be literal and, thus, cannot always be identified\nthrough specific keywords, making it highly context dependent. Deciding whether a post\ncontains hate speech, therefore, demands a more nuanced reading and empathy for the\nprotected group being targeted by a post. This suggests a level of subjectivity and a moral\ndimension (Lee, 2018) in hate speech moderation that aligns more readily with skills\nassociated with humans, than algorithms.\nThese task characteristics have consequences for perceptions of the fairness of and\ntrust in decision-makers (Lee, 2018). It can, thus, be assumed that perceptions of fairness\nand legitimacy as well as trust in a moderator vary with the type of content that is being\nscrutinized. Using this reasoning, algorithms would be preferred for objective tasks, such\nas recognizing and filtering profanity, and human moderators would be preferred for\nsubjective tasks, that is, recognizing and removing hate speech. Based on these
Gonçalves et al.
7
assumptions, we hypothesize the following interaction effect, positing that the type of\ncontent influences the differences in perceptions of human and algorithmic moderators:\nH4. The effect of moderator type depends on the type of content being moderated,\nsuch that human moderation is perceived as more trustworthy (H4a) and legitimate\n(H4b) and as having more procedural fairness (H4c) and outcome fairness (H4d)\nwhen removing hate speech when compared to removing profanity.\nWithin the scope of algorithm aversion and appreciation, it should be considered that\nthe ways in which AI is framed within national discourses may have an impact on how it\nits perceived. For instance, watching science fiction movies with AI weapon systems\nimpacts support for autonomous weapons (Young and Carpenter, 2018). While it is clear\nthat national differences must exist in how AI is framed within each of the countries in\nour study, we do not have information that would allow us to formulate expectations\nregarding specific country differences in perceptions. Data from the Eurobarometer\n(European Commission, 2019) on AI show that both the Portuguese and the Dutch are\namong those in Europe who agree the most that AI should be used for safety and security.\nSimilarly, the percentage of respondents in the United States who believe that AI requires\ncareful management is similar to the EU (European Union) average (Zhang and Dafoe,\n2019). Considering that no unambiguous differences in attitudes toward AI can be found\nfor the countries in our study, we focus our analytical strategy in testing whether effects\nhold across these national contexts rather than hypothesizing that effects are moderated\nby country.
Moderation messages\nWhen handling hateful content online, moderation messages matter. Users often feel frustrated and confused because they do not understand or agree with the reasons why content\nwas removed (Jhaver et al., 2019a). Even if bystanders agree with the need for content\nmoderation (Masip et al., 2019; da Silva, 2015), lack of transparency and engagement\nmay lead to myths and misconceptions regarding how content is handled by moderators\n(Suzor et al., 2019; Myers West, 2018). Users who participate in an online forum may\nsuddenly witness a situation where content they were engaging with disappears. While the\nuser who submitted the offending content may receive some explanation for its removal,\nother users are often simply confronted with a general message: “This comment was\nremoved by a moderator because it didn’t abide by our community standards” (example\nretrieved from The Guardian). The lack of specific information may in turn result in misconceptions and lack of support for moderation from these bystanders.\nOne answer to mitigate backfire effects of content moderation is to provide meaningful moderation messages to users. Giving additional information not only equates to\nmore transparency, but previous studies suggest that it may increase perceptions of fairness (Jhaver et al., 2019a). As such, we expect that increasing the amount of information\nin a custom moderation message, indicating the specific reason why content was\nremoved, will increase perceptions of fairness and transparency:
8
new media & society 00(0)\nH5. Custom messages for content removal are perceived as having more transparency\n(H5a), procedural fairness (H5b), and outcome fairness (H5c), when compared to the\nno information condition.
In addition to the direct effect of providing information about content removal, we\nalso expect to find interaction effects between our conditions. As mentioned above, moderating hate speech requires more contextual knowledge and is seen as more subjective\n(Lee, 2018). We, therefore, assume that hate speech content also increases the need for\nan explanation of why the content was removed:\nH6. The positive effect of providing a custom message compared to no information on\ntransparency (H6a), procedural fairness (H6b), and outcome fairness (H6c) is stronger\nfor hate speech than for profanity.\nRegarding the interaction between moderator type and explanations, previous research\nhas found that the source of the explanation had no impact on the future behavior of users\nwho have had their content removed (Jhaver et al., 2019c). However, different results\ncould occur when we consider perceptions rather than behavior as a dependent variable\nand when no explanation is provided. Based on the argument that algorithms are black\nboxes and generally less understandable for users than another person’s reasoning, we\nassume providing a reason as to why the content was removed will have a more positive\nimpact for an algorithm than for a human moderator:\nH7. The positive effect of providing a custom message compared to no information on\ntransparency (H7a), procedural fairness (H7b), and outcome fairness (H7c) is stronger\nfor the algorithmic moderator than for the human moderator.\nFinally, we also test the common scenario of online platforms presenting a general\nmoderation message that directs users to a community guidelines, policy or standards\npage. This approach provides more information than a simple content-removal message\nbut only if the user is willing to visit the community guidelines. On the contrary, not\nindicating the specific reason for content removal opens the possibility for differing\ninterpretations. While we expect that the general message condition will perform somewhere in the middle when compared with the remaining two explanations for removal\nconditions, we, nonetheless, ask the following research question:\nRQ1. Are user perceptions of a general moderation message (i.e. a reference to the\ncommunity guidelines) more similar to perceptions of a custom message (specific\nexplanation for the content removal) or to perceptions of receiving no explanatory\nmessage (no further explanation for the content removal)?\nTable 1 summarizes the hypothesized relationships. All hypotheses were preregistered, and decisions on directionality and dependent variable selection are grounded on\nthe previous literature.
Gonçalves et al.
9
Table 1. Research design.\nHypothesis
Independent variable
Directionality
Dependent variable
H1
+/−
H2
Hate speech (ref.\ncat. = Profanity)\nHuman (ref. cat. = AI)
H3
Human (ref. cat. = AI)
+
H4
Human (ref.\ncat. = AI) ×Hate speech\n(ref. cat. = Profanity)
+
H5
Specific information\n(ref. cat. = No\ninformation)
+
H6
Specific information\n(ref. cat. = No\ninformation) ×\nHate speech (ref.\ncat. = Profanity)\nSpecific information\n(ref. cat. = No\ninformation) ×\nHuman (ref. cat. = AI)
+
Procedural fairness (H1a)\nOutcome fairness (H1b)\nProcedural fairness (H2a)\nOutcome fairness (H2b)\nTrust (H3a)\nTransparency (H3b)\nLegitimacy (H3c)\nTrust (H4a)\nLegitimacy (H4b)\nProcedural fairness (H4c)\nOutcome fairness (H4d)\nTransparency (H5a)\nProcedural fairness (H5b)\nOutcome fairness (H5c)\nTransparency (H6a)\nProcedural fairness (H6b)\nOutcome fairness (H6c)
H7
+/−
−
Transparency (H7a)\nProcedural fairness (H7b)\nOutcome fairness (H7c)
Control variables\nCountry (ref.cat. = USA)\nAgreement with post\nAttitudes toward immigrants\nAI: artificial intelligence.
Method\nThis study was preregistered prior to data collection at OSF: https://osf.io/2n7d5/?view_\nonly=d029d9ffa052481f83eeb9e67717a8da. It was approved by institutional review\nboards both in Europe and in the United States.\nThe study consists of an online between-subjects experiment targeting residents in the\nUnited States, the Netherlands, and Portugal, with data collected in April 2020.\nParticipants were recruited from an Internet panel from Dynata. The final sample\n(n = 2870) was representative of the Internet users in each country for age, gender, education, and race/ethnicity.2 Details on the sample for each country are provided in\nSupplemental Appendix 3.3 Studying different national contexts is relevant because\napproaches on hate speech differ substantially in US and EU law (Bleich, 2014). In addition, even within the EU, countries are not homogeneous, underscoring the need to compare the Netherlands with Portugal to assess robustness across context. For example, in
10
new media & society 00(0)
Portugal censorship was common during the dictatorship that ended in 1974, and there\nare strong reactions to limiting speech, so residents there may perceive content moderation differently than in the Netherlands. In a similar way, understandings and discourses\nsurrounding AI may vary with national contexts as well.
Stimuli, design, and procedures\nThe stimuli for our study consists of two parts: a social media post with a short text with\noffensive content (either profanity or hate speech) and a content-removal message. A\ntotal of 10 messages (five profane and five hate speech) were pretested (n = 304) in the\nthree countries in our study. Two messages were selected for the final stimuli based on\ntheir perceived levels of profaneness and hatefulness, aiming at differentiating the two\nconditions as much as possible, that is, the hate speech message should be perceived as\nhateful but not profane and vice versa.4 Messages were translated and adapted to their\nrespective contexts. For instance, the hate speech message targeted Mexicans in the\nUnited States, Brazilians in Portugal, and foreigners in the Netherlands, to enhance the\nexternal validity of the study by referencing groups that are prominent targets of hate\nspeech in each country. The post was edited to mimic the popular online discussion template Disqus, with the prompt referencing a “post in a social media platform.” Because\nthis template is used in diverse discussion spaces, we ensure that our results are not\nimpacted by preconceptions that participants may have toward popular platforms, such\nas Facebook or Twitter. While not disclosing a specific platform limits the external validity of our findings, insofar that perceptions of moderation are always conditioned by the\nplatform where they take place, it allows us to measure baseline effects that could be\ninfluential across contexts.\nOur experiment followed a 2 (profanity vs hate speech) × 2 (algorithm vs human\nmoderator) × 3 (explanation for removal level of detail) design. Participants first saw a\nhate speech or profane message and were told that the message was posted on a social\nmedia site. They were then shown a message stating that the content had been removed,\nalong with a blurred version of the previous post to remove any ambiguity as to what\ncontent the message was referring to (see Supplemental Appendix 2). Our stimuli design\naims to mimic a situation that a frequent user of an online discussion platform with a\npost-moderation policy might encounter, where upon revisiting a particular discussion\nlater in time they might find that offending content has been removed. The content\nremoval message informs the participant that the social media post was removed because\nit breached the platform’s community guidelines, disclosing the moderator as human or\nalgorithm. To enhance the strength of our moderator manipulation, both the human moderator (John/Joost/Pedro) and the algorithm (ModerHate) were named. We decided to\npresent the human moderator as being a part of the platform’s content moderation team,\nrather than a volunteer moderator, to mimic the practices of some major social media\nplatforms like Facebook and Twitter. Although we acknowledge that some platforms,\nlike Reddit and Discord, are primarily moderated by volunteers, we address the implications of this distinction further in the “Discussion” section. The moderation message was\nmanipulated to disclose varying degrees of information to the user: stating that the content was removed because it violated the platform’s rules (no information condition);
Gonçalves et al.
11
directing the user to the actual community guidelines, with a working link to these guidelines (general message condition); or disclosing the specific reason (profanity or hate\nspeech) why the content was removed (custom message condition). After exposure to the\nmessage, questions related to the dependent variables followed, with control variable,\ndemographic, and manipulation checks at the end of the survey.\nRandom assignment was successful, with no significant differences across conditions\nin terms of age, gender, race/ethnicity, and education. Manipulation checks also indicate\nthat our manipulations of moderator type, χ2(3, N = 2817) = 832.25, p < .001, and message type χ2(4, N = 2739) = 519.68, p < .001, were successful.
Measures\nScales were adjusted to seven-point scales with answer options ranging from completely\ndisagree to completely agree. We used confirmatory factor analysis (CFA) to validate the\nstructure of our data. The measurement model displayed adequate fit considering the\nsample size: root mean square error of approximation (RMSEA) = 0.069 (90% confidence interval [CI] = [0.067, 0.071]) and comparative fit index (CFI) = 0.954 (χ2 = 3261.79,\ndf = 242, p < .001). After reliability checks, the measurement model was integrated in the\nstructural model described in the “Results” section. Some of the study researchers are\nfluent in all three languages (English, Dutch, and Portuguese) and translated stimuli and\nquestions. Although latent variables were used in the final model, means and standard\ndeviations of averaged items are presented below for context.\nOutcome fairness: Based on Colquitt (2001), we used four items to assess participant’s perceptions of outcome fairness (e.g. “The moderator’s decision is appropriate in\nregard to the post” and “I think it was fair to remove the post.”) (M = 5.45; SD = 1.69;\nα = .96).\nProcedural fairness: Drawing from Colquitt (2001) and Koper et al. (1993), we used\nfive statements to assess perception of the fairness of the moderation process (e.g. “The\nmoderation process has followed ethical and moral standards.”) (M = 5.43; SD = 1.41;\nα = .93).\nLegitimacy: We adapted a legitimacy scale from van der Toorn et al. (2011), using five\nstatements to measure the legitimacy of the moderator and their content moderation\npractices (e.g. “The moderator has the right to evaluate posts that users make on this\nsocial networking site.”) (M = 5.16; SD = 1.29; α = .86).\nTransparency: Transparency was assessed by adapting and using the scores of five\nitems drawn from Rawlins (2008) to measure how transparent the participants considered the moderation process to be (e.g. “Users like me are provided with detailed information to understand the removal of the post.”) (M = 5.03; SD = 1.49; α = .95).\nTrust: Our five-item scale for trust was adapted from Rawlins (2008) and Ohanian\n(1990). The items aimed to capture the extent to which participants trusted the moderator\n(e.g. “I think the moderator is honest when evaluating posts on this social networking\nsite.”) (M = 5.27; SD = 1.29; α = .92)\nAttitudes toward immigrants: Because our hate speech condition directs offensive\ncomments at immigrants, it was important to control for this factor. To ensure a robust\nsix-item scale to measure these attitudes, two items were drawn from Pew Research
12
new media & society 00(0)
Table 2. Model fit statistics.
RMSEA [90% CI]\nCFI\nSRMR\nχ2\ndf\nAIC
Model 1
Model 2
Model 3
Model 4
.05 [.05, .05]\n.95\n.03\n3453.74***\n413\n174,750.21
.05 [.05, .05]\n.95\n.03\n3769.01***\n451\n174,567.64
.05 [.05, .05]\n.95\n.03\n3839.75***\n489\n172,565.236
.05 [.04, .05]\n.95\n.02\n3971.45***\n641\n172,601.38
RMSEA: root mean square error of approximation; CFI: comparative fit index; SRMR: standardized root\nmean residual; AIC: Akaike information criterion.\nModel 1: Experimental conditions and interactions between conditions.\nModel 2: Experimental conditions, interactions between conditions, and country control.\nModel 3: Experimental conditions, interactions between conditions, and all controls.\nModel 4: Experimental conditions, interactions between conditions, all controls, and interaction between\ncountry and conditions.\n***p < .001.
Center polling (Jones, 2019), two from the American National Election Studies, and two\nfrom Gallup polling. Three of the items are reversed, and the statements were adapted for\neach national context (e.g. “American identity, norms and values have been enriched\nthanks to the presence of immigrants.”) (M = 4.29; SD = 1.36; α = .92)\nAgreement with post: Because people who agree with a post would be less likely to\nsupport its removal, we also controlled for this in our models. Agreement was measured\nby asking participants how much they agreed with the social media post (M = 3.33;\nSD = 2.09).
Model selection\nTo test the hypotheses, we used structural equation modeling (SEM) with the lavaan\n(Rosseel, 2012) package in R. SEM tackles the complexity of our research design while\nallowing us to integrate the analyses in a single research model. The latent dependent\nvariables were included in the model as specified in the measurement model (see\nSupplemental Appendix 1 for details), while items on attitudes toward immigrants were\naveraged before their inclusion in the model.\nTheoretically, we expected that control variables would improve our model. Country\ndifferences, attitudes toward immigrants, and agreement with the post should all have an\nimpact on our dependent variables. However, we did not hypothesize interactions\nbetween our experimental conditions and controls because we did not have specific\nexpectations that the effect of our manipulations would depend on country and attitudes.\nEmpirically, we compared four different model specifications to assess whether adding\nour control variables was justified (comparing Models 2 and 3 with Model 1), and\nwhether adding country interactions would result in an improved fit. Table 2 indicates\nthat model fit statistics support our theoretical expectations. There are no meaningful\ndifferences in fit statistics between the four models, but the Akaike information criterion
Gonçalves et al.
13
Table 3. Structural equation model.
Hate speech\nHuman\nGeneral message\nCustom message\nHate speech × human\nGeneral message × human\nCustom message × human\nHate speech × general\nmessage\nHate speech × custom\nmessage\nAgreement with post\nAttitudes immigrants\nNetherlands\nPortugal
Outcome fairness
Procedural fairness Legitimacy
Transparency
Trust
0.92 (0.11)***\n−0.02 (.11)\n−0.28 (0.12)*\n−0.05 (0.12)\n−0.05 (0.11)\n0.29 (0.14)*\n0.22 (.14)\n−0.00 (0.14)
0.59 (0.10)***\n−0.06 (.10)\n−0.17 (0.10)\n0.03 (0.10)\n−0.02 (0.10)\n0.20 (0.12)\n0.19 (.12)\n−0.01 (0.12)
0.45 (0.10)***\n0.02 (.10)\n−0.16 (0.10)\n0.01 (0.10)\n−0.05 (0.10)\n0.18 (0.12)\n0.13 (.12)\n0.04 (0.12)
0.37 (0.11)**\n−0.35 (.11)**\n0.04 (0.11)\n0.41 (0.11)***\n0.18 (0.11)\n0.28 (0.13)*\n0.33 (.13)*\n0.12 (0.13)
0.29 (0.09)**\n−0.07 (.09)\n−0.26 (0.10)**\n−0.08 (0.10)\n0.04 (0.09)\n0.19 (0.11)\n0.24 (.11)*\n0.18 (0.11)
−0.06 (0.14)
−0.11 (0.12)
−0.00 (0.12)
−0.09 (0.13)
0.05 (0.11)
−0.13 (0.01)***\n0.18 (0.02)***\n0.34 (0.07)***\n0.67 (0.07)***
−0.10 (0.01)***\n0.16 (0.02)***\n0.30 (0.06)***\n0.61 (0.06)***
−0.08 (0.01)***\n0.13 (0.02)***\n0.33 (0.06)***\n0.41 (0.06)***
−0.04 (0.01)**\n0.12 (0.02)***\n–0.12 (0.07)\n0.20 (0.07)**
−0.06 (0.01)***\n0.13 (0.02)***\n0.28 (0.06)***\n0.52 (0.06)***
AI: artificial intelligence.\nValues in each cell are presented as unstandardized estimate (standard error).\nReference category for hate speech is profanity, reference category for human is AI, reference category for general and\ncustom message is no information, and reference category for country is the United States.\n***p < .001, **p < .01, *p < .05.
(AIC) suggests that Model 3, having the experimental conditions, interactions between\nconditions and controls as predictors for our five dependent variables, is superior. Fit\nstatistics indicated an adequate fit of the model to our data: RMSEA = 0.051 (90%\nCI = [0.050, 0.053]) and CFI = 0.950 (χ2 = 3839.75, df = 489, p < .001), even though the\nchi-square test is significant due to our large sample size.5
Results\nWhile SEM models are usually represented visually, the size and complexity of our\nmodel make it easier to display the results in a tabular form (Table 3). In support of H1,\nwhich predicted differences in perceived procedural fairness (1) and outcome fairness\n(2) between removal of hate speech and profanity content, we see that the type of content\nbeing moderated has an impact on both dependent variables. In fact, outcome fairness\n(US ΔM = 1.24; NL ΔM = 1.13; PT ΔM = 0.80, all p < .001) and procedural fairness (US\nΔM = 0.84; NL ΔM = 0.71; PT ΔM = 0.49, all p < .001) are consistently higher on the hate\nspeech condition for all three countries.\nIn contrast, there were no significant differences in outcome or procedural fairness\nbetween the two forms of human and algorithmic moderation tested in our study, thus\nrejecting H2. Addressing H3, which predicted that algorithmic moderators would be perceived as (a) less trustworthy, (b) transparent, and (c) legitimate than human moderators,\nwe found no significant effects of moderator type on legitimacy and trust. However, for\nH3b, we found the opposite effect of what we predicted: The algorithmic moderator was
14
new media & society 00(0)
rated as more transparent than the human moderator (B = −.35, p = .001), not less. H3 is\nrejected.\nH4 predicted an interaction between content type and moderator type, such that\nhumans are seen as more just when removing hate speech. However, as shown in Table\n3, none of the interaction terms involving content type was significant. This means that\nalthough there was a direct effect of content type on our dependent variables, this variable did not condition participants’ preference for a human or algorithmic moderator, at\nleast in the form of moderation tested in our experiment. Thus, H4 is rejected.\nH5 stated that custom messages for content removal have higher procedural fairness\n(H5a), outcome fairness (H5b), and transparency (H5c), when compared to the no information condition. It is also related to content removal explanations. RQ1 asked whether\nuser perceptions of general moderation message were closer to a custom message or to\nreceiving no information in the message. Regarding H5, a custom message, indicating\nthe specific reason why content was removed, only performed better than a no information message in terms of transparency. Therefore, the data support H5c, but H5a and H5b\nare rejected. Regarding RQ1, it is interesting to note that a general message, represented\nby the common practice of directing the users to community guidelines, is outperformed\nin terms of outcome fairness and trust by the no information condition. When probing the\nsignificant interaction term for outcome fairness between the general message and the\nmoderator type conditions, we see that the low performance of the general message condition happens mostly in the AI condition.\nH6 posited that that there would be an interaction between message type and content\ntype, such that an explanation would have a stronger positive impact on fairness when\nremoving hate speech when compared to profanity. As mentioned, none of the interaction terms containing the content type condition were significant, meaning that H6 is\nrejected. In turn, H7 hypothesized a significant interaction effect for the moderator type\nand message type conditions, such that algorithms would benefit more from the effect of\nproviding a custom message. Table 3 shows a significant interaction effect for transparency in this regard, illustrated through the estimated marginal means in Figure 1.\nHowever, as shown by the estimated marginal means, providing a custom message benefits the transparency of a human moderator more than an algorithmic moderator, particularly when compared to the no information condition. This goes against our initial\nexpectation of AI benefiting more from additional information, and it means that H7 is\nrejected.\nFinally, even though we tested a specific form of moderation, it should be noted that\nthe conclusions drawn are robust across different countries. Not only did Model 4 (with\ncountry interactions) have worse fit statistics, but also most interaction effects between\nexperimental conditions and country were nonsignificant. The only exception was the\ninteraction between Portugal and the hate speech condition, which had a significant negative effect on outcome fairness (B = −.36, p = .011), procedural fairness (B = −.29,\np = .015), and legitimacy (B = −.36, p = .002). This is in line with the results from our\npretest, which indicated that the distinction between profanity and hate speech is less\nclear for the Portuguese.
Gonçalves et al.
15
5.4\n5.326
5.3
5.257
5.2\n5.1\n5.015
5\n4.9
4.902
5.003
4.8\n4.7\n4.6
4.66\nNo informaon
General message\nAI
Custom message
Human
Figure 1. Estimated marginal means for transparency.
Values for attitude toward immigrants held constant at 4.29 and values for agreement with post held constant at 3.33.
Discussion\nWe explored how different factors affect perceptions of online content moderation and\nwhether these effects were robust across countries. By doing so, our findings contribute\nto theories on algorithmic aversion (Dietvorst et al., 2015) and hate speech processing\n(Kenski et al., 2017), and provide practical insights into online platforms for handling\nhate speech and profanity.\nConcerning algorithmic aversion, our study shows that there are significant differences in terms of how individuals perceive the forms of human and AI moderation tested\nby our experiment, but these differences are far from linear. While previous studies suggested that the type of task influences how people perceive algorithms (Castelo et al.,\n2019), our study concludes that the type of content being deleted (profanity vs hate\nspeech) has no effect on perceptions of AI moderators. Our findings suggest that it is\nlikely that bystanders to content moderation see it as a single task, and do not believe that\nhumans or algorithms are more suitable to moderate specific types of content in settings\nsimilar to our experiment. While our stimuli exposed participants to a situation that is\ncommonly encountered online, it should be noted that differences between moderator\ntypes may emerge in contexts where the outcome is uncertain or unknown, unlike our\nsetting where a decision was presented to participants. Studies on algorithmic aversion\nhave shown that the actual performance of an algorithm is key to perceptions (Castelo\net al., 2019; Dietvorst et al., 2015). Our study presented participants with a situation\nwhere an algorithm correctly identified the presence of hate speech and profanity in a\ntext, which may have reinforced their beliefs of the capabilities of AI. While this is true\nfor all conditions, if the outcome were not known, it would be possible that participants\nwould show a preference toward one of the moderator types, since algorithms tend to be\npenalized more heavily by mistakes. Likewise, it should be noted that, while these
16
new media & society 00(0)
findings apply to bystanders, one should be cautious in generalizing these findings to\nindividuals who were personally subjected to moderation. Specifically, due to the thirdperson effect (Davison, 1983), bystanders may be more inclined to act against objectionable content than the publishers of the said content, a hypothesis that is supported by\nfindings regarding Internet pornography (Sun et al., 2008). This is especially relevant\nconsidering studies that show how the third-person effect also applies to hate speech\nperceptions (Guo and Johnson, 2020).\nAlthough content type was not a significant moderator in this context, our findings\nhighlight how algorithmic perceptions depend on how a decision is communicated and\non the dependent variable being measured. For transparency, our study found evidence\nfor algorithmic appreciation, especially when no additional information on the decision\nis given to the participant. While algorithms are generally assumed to be black boxes, no\nprevious algorithm aversion studies that we are aware of have transparency as a dependent variable to verify this (Castelo et al., 2019). The fact that this difference only occurred\nfor the no information condition hints that, when provided with additional information\non why the decision was made, information supersedes moderator type and erodes differences in perceived transparency. This finding aligns with the results from Jhaver et al.\n(2019c), who found no differences in posting behavior between users who received content moderation explanations by algorithms or volunteer humans.\nOne important caveat regarding our findings on the role of human moderators is that our\nstudy operationalized human moderation being enforced by a platform’s employee and not\nby volunteer moderators, who are the focus of other studies on content moderation such as\nthe one referenced above (Jhaver et al., 2019c). Volunteer moderators may have more\nautonomy regarding content deletion decisions, and research shows that their views on\ntransparency are not uniform (Juneja et al., 2020), which means that perceptions regarding\nthis type of moderation may also be distinct from what we encountered. In addition, volunteer moderators may have a relationship with community members that hired moderators\nlack, influencing dimensions such as trust and legitimacy from those community members.\nRegarding our operationalization of moderation, it should also be noted that while we\nchose to clearly distinguish between human and AI moderation, actual practices, like\nReddit’s Automoderator (Jhaver et al., 2019b), may embody a collaboration between\nhuman volunteers and AI tools, effectively introducing a hybrid type of content\nmoderation.\nThe importance of accounting for different dependent variables and information conditions when assessing algorithm aversion is strengthened by our finding that AI has\nlower outcome fairness than humans, when users are directed toward community guidelines. This difference is not unexpected because previous research indicates that algorithmic aversion is partly due to the additional cognitive load required to follow\nalgorithmic advice when compared to humans (Burton et al., 2020). However, most\nprevious studies had different processes associated with following human or algorithmic advice, while our study kept the actual work required from the user consistent\n(visiting the community guidelines). A plausible explanation for our finding, therefore,\nlies in expectancy violation theory (Burgoon, 1993; Kizilcec, 2016). One key advantage\nthat algorithms seem to have related to human agents lies in their perceived personalization (Eslami et al., 2018). However, sending a user to general guidelines disrupts that
Gonçalves et al.
17
expectation of personalization, therefore, erasing the advantage that algorithms would\nhave in terms of outcome fairness.\nOur study not only highlights how perceptions on moderators and moderation depend\non how decisions are communicated, but also that the direction of the effects is contingent on the dependent variable that is measured. The opposite effects found in our study\nillustrate why different scholars found evidence for both algorithm aversion (Dietvorst\net al., 2015) and appreciation (Logg et al., 2019), and why the discussion on perceptions\nof AI is more complex than one agent simply outperforming the other.\nConcerning the differences between deleting online hate speech and profanity, our\nresults show that deleting the former is clearly seen as more just. Like most findings in\nour study, this difference is consistent across all three countries, despite distinct legal\nframeworks. This is an encouraging finding because scholars (Papacharissi, 2004) tend\nto see hate speech as more harmful for democracy than profane language. While hate\nspeech may be in the eye of the beholder (Roussos and Dovidio, 2018), users do make a\ndistinction between this type of content and other forms of harmful speech when assessing what is permissible online.\nWhile specific to the form of content moderation that was tested, our findings mean\nthat transitioning to AI-reliant content moderation systems does not necessarily come at\na cost to user perceptions, at least for bystanders to moderation. In fact, AI moderation\nmay be beneficial in terms of transparency when it is not feasible to provide users with\nadditional information about why content is removed. However, practitioners using AI\nshould be wary of sending users to a community guidelines page because this had detrimental consequences for how bystanders perceived the fairness of moderation. Due to\nthe severity of hate speech, any change in these outcomes, even if small and with limited\nexternal validity, may be consequential. We believe community managers, platforms,\nand policymakers should carefully consider any contextual factors that make the presence of hate speech online more acceptable. Finally, our results should be considered in\nlight of the social and historical contexts of each platform, since these can have a significant impact on how users perceive moderation (Schoenebeck et al., 2020). While we\nchose not to disclose any specific platform in our experiment, practitioners should also\nreflect on the perceptions of their users and how previous interactions may condition\ntheir engagement with moderation systems.
Limitations and future research\nWe examine participant perceptions regarding content removal from the perspective of a\nbystander; therefore, while it is tempting to generalize these findings to those who personally have had their hateful or profane content removed, effects may be stronger, and\neven distinct in directionality for these cases. However, operationalizing first-person\neffects in a controlled between-subjects experimental setting with random assignment,\nwhile possible, would have been particularly challenging due to external validity (i.e.\nreplicating the natural circumstances where participants would post hateful messages)\nand ethical concerns (i.e. prompting participants to produce hate speech messages).\nNevertheless, we are encouraged by the fact that some of our findings, such as the lack\nof difference in perceived transparency when explanations are given by humans and AI,
18
new media & society 00(0)
are aligned with results that looked at behavior on actual platforms like Reddit (Jhaver\net al., 2019c). Furthermore, while online survey experiments on perceptions of the\nremoval of one’s own content may be difficult to operationalize, online field experiments\ntesting similar factors and perceptions are possible and yield relevant results (Marias and\nMou, 2018). In our case, the bystander perspective allows us to contribute to the literature on algorithmic aversion, by looking at perceptions of algorithms performing a task\nthat has been increasingly delegated to them. In addition, the success of content moderation systems in curtailing the spread of online hate speech also depends on a general\nsupport for these systems, which cannot be limited to users who had firsthand experience\nwith content deletion. Given the severity of the hateful content in our experiment, any\nshifts in support for moderation in this instance should be considered carefully.\nIn addition, it is important to note that the results of this study, despite its crossnational component, are time and context bound. As technical developments and popular\nculture portrayals of AI develop, the way individuals perceive and relate to machine\nlearning is likely to change, including content management and moderation. However,\nwhile work on algorithmic bias (Raji and Buolamwini, 2019) and documentaries such as\nCoded Bias may eventually lead to changes in perceptions, this article provides not only\na relevant baseline for future longitudinal studies, but also contributes to unveiling some\nof the theoretical mechanisms that may influence how perceptions and engagement with\nAI content moderation develops.\nOur findings regarding multiple dependent variables and the wording of moderation\nexplanations can be used by other scholars to further explore the nuances in algorithmic\naversion and appreciation in specific contexts and platforms. A research design that\nincorporates mediation and causality, for instance, would be able to clarify the role of\ntransparency and trust have in perceptions of decision-making outcomes. Overall, even\nthough our study only tested a limited set of the many forms that content moderation can\ntake, our findings suggest that future research in this strand should consider the implications of dependent variable selection and wording of the stimuli.\nDeclaration of conflicting interest\nThe author(s) declared the following potential conflicts of interest with respect to the research,\nauthorship, and/or publication of this article: This study was made possible through a Facebook\nresearch award. Since the topic of the study relates directly to Facebook’s activity, there could be\na potential conflict of interest. However, Facebook was not involved at any stage of the design,\nexecution, or reporting of the study. The funds were awarded as an unrestricted gift, and all\nresearch was carried out independently by the authors without any external interference or\ninfluence.
Funding\nThe author(s) disclosed receipt of the following financial support for the research, authorship, and/\nor publication of this article: The authors received the financial support from Facebook for this\nproject.
Gonçalves et al.
19
ORCID iDs\nJoão Gonçalves\nhttps://orcid.org/0000-0002-8948-0455\nIna Weber\nhttps://orcid.org/0000-0002-3501-6905\nGina M Masullo\nhttps://orcid.org/0000-0002-4909-2116\nJoep Hofhuis\nhttps://orcid.org/0000-0001-7531-8644
Supplemental material\nSupplemental material for this article is available online.
Notes\n1.\n2.
3.
4.
5.
For instance, according to the Facebook Community Standards Enforcement Report for Q4,\nless than 0.1% of posts are removed because of hate speech.\nRace/ethnicity questions were not asked to the Portuguese since these are forbidden by the\nPortuguese Constitution. In the Netherlands, the country of birth of the mother/father was\nasked as proxy for race and ethnicity, since US categories do not translate well to this context.\nA total of 4603 participants filled in the survey. Participants who did not answer (n = 615) or\nwho failed the attention check (n = 878), completed the survey in less than 48% of the median\nduration (n = 213), and straight-liners (n = 27) were excluded from the analysis. This resulted\nin a sample of 2870 participants, (n = 902 in the United States; n = 975 in the Netherlands; and\nn = 993 in Portugal).\nRespondents were asked the extent to which each of 10 pretested messages contained profanity or hate speech. The message selected for the profanity condition was rated as significantly\n(p < .001) more profane (M = 3.95) than the message for the hate speech condition (M = 3.12).\nSimilarly, participants reported a significantly (p < .001) higher presence of hate speech for\nthe message of the hate speech condition (M = 4.35) than for the message on the profanity\ncondition (M = 3.66).\nThe only significant change in estimates between Model 3 and a model accounting for interactions between the conditions and the control variables is that the negative effect of a general message on procedural fairness is significant with p = .022. All other significance levels\n(α = .050) and effect directions for the experimental conditions and their interactions are\nrobust across models.
References\nBandura A (2005) The evolution of social cognitive theory. In: Smith KG and Hitt MA (eds) Great\nMinds in Management. Oxford: Oxford University Press, pp. 9–35.\nBleich E (2014) Freedom of expression versus racist hate speech: explaining differences between\nhigh court regulations in the USA and Europe. Journal of Ethnic and Migration Studies 40(2):\n283–300.\nBrown A (2017) What is so special about online (as compared to offline) hate speech? Ethnicities\n18(3): 297–326.\nBurgoon JK (1993) Interpersonal expectations, expectancy violations, and emotional communication. Journal of Language and Social Psychology 12(1–2): 30–48.\nBurton JW, Stein MK and Jensen TB (2020) A systematic review of algorithm aversion in augmented decision making. Journal of Behavioral Decision Making 33(2): 220–239.\nCastelo N, Bos MW and Lehmann DR (2019) Task-dependent algorithm aversion. Journal of\nMarketing Research 56(5): 809–825.\nChen GM (2017) Online Incivility and Public Debate: Nasty Talk. New York: Palgrave Macmillan.
20
new media & society 00(0)
Coe K, Kenski K and Rains SA (2014) Online and uncivil? Patterns and determinants of incivility\nin newspaper website comments. Journal of Communication 64: 658–679.\nColquitt JA (2001) On the dimensionality of organizational justice: A construct validation of a\nmeasure. Journal of Applied Psychology 86(3): 386–400.\nColquitt JA, Greenberg J and Zapata-Phelan CP (2005) What is organizational justice? A historical\noverview. In: Greenberg J and Colquitt JA (eds) Handbook of Organizational Justice. New\nYork: Lawrence Erlbaum Associates, pp. 3–56.\nda Silva MT (2015) What do users have to say about online news comments? Readers’ accounts\nand expectations of public debate and online moderation: A case study. Participations:\nJournal of Audience and Reception Studies 12: 32–44.\nDavison WP (1983) The third-person effect in communication. Public Opinion Quarterly 47(1):\n1–15.\nDiab DL, Pui SY, Yankelevich M, et al. (2011) Lay perceptions of selection decision aids in U.S.\nand non-U.S. samples. International Journal of Selection and Assessment 19(2): 209–216.\nDietvorst BJ, Simmons JP and Massey C (2015) Algorithm aversion: People erroneously avoid\nalgorithms after seeing them err. Journal of Experimental Psychology: General 144: 114–\n126.\nDjuric N, Zhou J, Morris R, et al. (2015) Hate speech detection with comment embeddings. In:\nProceedings of the 24th international conference on World Wide Web, Florence, 18–22 May\n2015, pp. 29–30. New York: ACM.\nECRI (2016) ECRI general policy recommendation no.15 on combating hate speech. Council of\nEurope. Available at: https://rm.coe.int/ecri-general-policy-recommendation-no-15-on-combating-hate-speech/16808b5b01 (accessed June 2021).\nErjavec K and Kovačič MP (2012) “You don’t understand, this is a new war!” Analysis of hate\nspeech in news web sites’ comments. Mass Communication and Society 15(6): 899–920.\nEslami M, Krishna Kumaran SR, Sandvig C, et al. (2018) Communicating algorithmic process in\nonline behavioral advertising. In: Proceedings of the 2018 CHI conference on human factors\nin computing systems, Montreal, QC, Canada, 21–26 April 2018, pp. 1–13. New York: ACM.\nEuropean Commission (2019) Standard Eurobarometer 92: Europeans and artificial intelligence.\nAvailable at: http://ec.europa.eu/commfrontoffice/publicopinion (accessed June 2021)\nEuropean Commission (2020) The EU Code of conduct on countering illegal hate speech online.\nAvailable at: https://ec.europa.eu/info/policies/justice-and-fundamental-rights/combattingdiscrimination/racism-and-xenophobia/eu-code-conduct-countering-illegal-hate-speechonline_en (accessed June 2021)\nFacebook (2020) Hate speech. Available at: https://www.facebook.com/communitystandards/\nhate_speech (accessed June 2021).\nGagliardone I (2019) Defining online hate and its “public lives”: What is the place for “extreme\nspeech”? International Journal of Communication 13: 3068–3087.\nGillespie T (2018) Custodians of the Internet: Platforms, Content Moderation and the Hidden\nDecisions That Shape Social Media. New Haven, CT: Yale University Press.\nGoodman E and Cherubini F (2013) Online Comment Moderation: Emerging Best Practices.\nFrankfurt: World Association of Newspapers and News Publishers.\nGuo L and Johnson BG (2020) Third-person effect and hate speech censorship on Facebook.\nSocial Media + Society 6(2): 1–12.\nHalfaker A, Geiger RS, Morgan JT, et al. (2012) The rise and decline of an open collaboration\nsystem: how Wikipedia’s reaction to popularity Is causing its decline. American Behavioral\nScientist 57(5): 664–688.
Gonçalves et al.
21
Helberger N, Araujo T and de Vreese CH (2020) Who is the fairest of them all? Public attitudes\nand expectations regarding automated decision-making. Computer Law & Security Review\n39: 105456.\nJhaver S, Appling DS, Gilbert E, et al. (2019a) “Did you suspect the post would be removed?”\nUnderstanding user reactions to content removals on Reddit. In: Proceedings of the ACM on\nhuman-computer interaction, Glasgow, 4–7 May 2019, pp. 1–33. New York: ACM.\nJhaver S, Birman I, Gilbert E, et al. (2019b) Human-machine collaboration for content regulation:\nThe case of Reddit automoderator. ACM Trans. Computer-human Interactions 26(5): 1–35.\nJhaver S, Bruckman A and Gilbert E (2019c) Does transparency in moderation really matter?:\nUser behavior after content removal explanations on reddit. In: Proceedings of the ACM on\nHuman-computer Interaction, Glasgow, 4–7 May 2019, pp. 1–27. New York: ACM.\nJones B (2019) Majority of Americans continue to say immigrants strengthen the U.S. Pew Reseach,\nPew Research. Available at: https://www.pewresearch.org/fact-tank/2019/01/31/majority-ofamericans-continue-to-say-immigrants-strengthen-the-u-s/ (accessed June 2021).\nJuneja P, Subramanian DR and Mitra T (2020) Through the looking glass: Study of transparency\nin Reddit’s moderation practices. In: Proceedings of the ACM Human-computer Interactions,\nHonolulu, HI (Cancelled Due to COVID)19, pp. 1–35. New York: Association for Computing\nMachinery.\nKantayya S (2020) Coded Bias. Netflix.\nKenski K, Coe K and Rains SA (2017) Perceptions of uncivil discourse online: an examination of types and predictors. Communication Research 47(6): 795–814. https://doi.\norg/10.1177/0093650217699933\nKizilcec RF (2016) How much information? Effects of transparency on trust in an algorithmic\ninterface. In: Proceedings of the 2016 CHI conference on human factors in computing systems, San Jose, CA, 7–12 May 2016, pp. 2390–2395. New York: ACM.\nKoper G, Van Knippenberg D, Bouhuijs F, et al. (1993) Procedural fairness and self-esteem.\nEuropean Journal of Social Psychology 23(3): 313–325. https://doi.org/10.1002/\nejsp.2420230307\nKrämer B and Springer N (2020) Ontology of opposition online: Representing antagonistic structures on the Internet. Studies in Communication and Media 9(1): 35–61.\nLee MK (2018) Understanding perception of algorithmic decisions: Fairness, trust, and emotion in\nresponse to algorithmic management. Big Data & Society 5(1): 1–16.\nLogg JM, Minson JA and Moore DA (2019) Algorithm appreciation: People prefer algorithmic\nto human judgment. Organizational Behavior and Human Decision Processes 151: 90–103.\nLongoni C, Bonezzi A and Morewedge CK (2019) Resistance to medical artificial intelligence.\nJournal of Consumer Research 46(4): 629–650.\nMasip P, Ruiz-Caballero C and Suau J (2019) Active audiences and social discussion on the digital\npublic sphere. Review article. El Profesional De La Información 28(2): 1–40.\nMarias JN and Mou M (2018) Civil Servant: community-led experiments in platform governance. In: Proceedings of the 2018 CHI conference on human factors in computing systems,\nMontreal, Qanada, 21–26 April 2018, pp. 1–13. New York, NY: Association for Computing\nMachinery. https://doi.org/10./3173574.3173583.\nMuddiman A and Stroud NJ (2017) News values, cognitive biases, and partisan Incivility in comment sections. Journal of Communication 67: 586–609.\nMyers West S (2018) Censored, suspended, shadowbanned: user interpretations of content moderation on social media platforms. New Media & Society 20(11): 4366–4383.\nNaab TK, Kalch A and Meitz TGK (2016) Flagging uncivil user comments: effects of intervention\ninformation, type of victim, and response comments on bystander behavior. New Media &\nSociety 20(2): 777–795.
22
new media & society 00(0)
Ötting SK and Maier GW (2018) The importance of procedural justice in human–machine interactions: intelligent systems as new decision agents in organizations. Computers in Human\nBehavior 89: 27–39.\nOhanian R (1990) Construction and validation of a scale to measure celebrity endorsers’ perceived\nexpertise, trustworthiness, and attraction. Journal of Advertising 19(3): 39–52.\nPapacharissi Z (2002) The virtual sphere: the internet as a public sphere. New Media & Society 4:\n9–27.\nPapacharissi Z (2004) Democracy online: civility, politeness, and the democratic potential of\nonline political discussion groups. New Media & Society 6: 259–283.\nPaul K (2020) Zuckerberg: Facebook will review policies after backlash over Trump posts. The\nGuardian, 6 June. Available at: https://www.theguardian.com/technology/2020/jun/05/markzuckerberg-facebook-trump-policies-review\nPohjonen M (2019) A comparative approach to social media extreme speech: online hate speech as\nmedia commentary. International Journal of Communication 13: 3088–3103.\nRaji I and Buolamwini J (2019) Actionable auditing: investigating the impact of publicly naming\nbiased performance results of commercial AI products. In: conference on artificial intelligence, ethics, and society, Honolulu, HI, 27–28 January 2019, Palo Alto, CA: Association for\nthe Advancement of Artificial Intelligence. https://hdl.handle.net/1721.1/123456\nRawlins B (2008) Measuring the relationship between organizational transparency and employee\ntrust. Public Relations Journal 2(2): 1–21.\nRichardson-Self L (2018) Woman-hating: on misogyny, sexism, and hate speech. Hypatia 33(2):\n256–272.\nRiedl MJ, Whipple KN and Wallace R (2021) Antecedents of support for social media content moderation and platform regulation: the role of presumed effects on self and others. Information, Communication & Society. Epub ahead of print 26 January 2021. DOI:\n10.1080/1369118X.2021.1874040\nRosseel Y (2012) lavaan: an R package for structural equation modeling. Journal of Statistical\nSoftware 48(2): 1–36. http://www.jstatsoft.org/v48/i02/\nRoussos G and Dovidio JF (2018) Hate speech is in the eye of the beholder: the influence of racial\nattitudes and freedom of speech beliefs on perceptions of racially motivated threats of violence. Social Psychological and Personality Science 9(2): 176–185.\nSchoenebeck S, Haimson OL and Nakamura L (2020) Drawing from justice theories to support\ntargets of online harassment. new media & society 23(5): 1278–1300.\nSELMA (2019) Hacking Online Hate: Building an Evidence Base for Educators. Available at:\nwww.hackinghate.eu (accessed June 2021)\nShin D and Park YJ (2019) Role of fairness, accountability, and transparency in algorithmic\naffordance. Computers in Human Behavior 98: 277–284.\nSoral W, Bilewicz M and Winiewski M (2018) Exposure to hate speech increases prejudice\nthrough desensitization. Aggressive Behavior 44(2): 136–146.\nSun Y, Shen L and Pan Z (2008) On the behavioral component of the third-person effect.\nCommunication Research 35(2): 257–278.\nSuzor NP, West SM, Quodling A, et al. (2019) What do we mean when we talk about transparency? Toward meaningful transparency in commercial content moderation. International\nJournal of Communication 13: 1526–1543.\nTyler T, Katsaros M, Meares T, et al. (2021) Social media governance: can social media companies motivate voluntary rule following behavior among their users? Journal of Experimental\nCriminology 17(1): 109–127.\nTyler TR (2006) Psychological perspectives on legitimacy and legitimation. Annual Review of\nPsychology 57(1): 375–400.
Gonçalves et al.
23
van der Toorn J, Tyler TR and Jost JT (2011) More than fair: outcome dependence, system justification, and the perceived legitimacy of authority figures. Journal of Experimental Social\nPsychology 47(1): 127–138.\nvan Dijke M, De Cremer D and Mayer DM (2010) The role of authority power in explaining procedural fairness effects. Journal of Applied Psychology 95(3): 488–502.\nYoung KL and Carpenter C (2018) Does science fiction affect political fact? Yes and no: a survey\nexperiment on “killer robots..” International Studies Quarterly 62(3): 562–576.\nZhang B and Dafoe A (2019) Artificial Intelligence: American Attitudes and Trends. Center for the\nGovernance of AI, Future of Humanity Institute, University of Oxford. Available at: https://\ngovernanceai.github.io/US-Public-Opinion-Report-Jan-2019/index.html (accessed June\n2021).\nZhang Z, Robinson D and Tepper J (2018) Detecting hate speech on Twitter using a convolutionGRU based deep neural network. In: European semantic web conference, Crete, 3–7 June\n2018. Cham: Springer.
Author biographies\nJoão Gonçalves is an assistant professor (tenured) at the Department of Media & Communication\nof Erasmus University Rotterdam. He obtained his PhD in Communication Studies from the\nUniversity of Minho (Portugal), and his research focuses on artificial intelligence, audience\nengagement, and political communication.\nIna Weber is a doctoral student at the Department of Communication Studies at the University of\nAntwerp. Her research focuses on the risk factors that facilitate hate speech and how to use technology to mitigate these risks. She obtained her MA degree in Media, Culture, and Society from\nErasmus University Rotterdam.\nGina M. Masullo is an associate professor in the School of Journalism and Media and Associate\nDirector of the Center for Media Engagement in the Moody College of Communication at The\nUniversity of Texas at Austin. Her research focuses on how the digital space both connects and\ndivides people and how that influences society, individuals, and journalism.\nMarisa Torres da Silva is an assistant professor (tenured) at the School of Social Sciences and\nHumanities, Universidade Nova de Lisboa (NOVA FCSH). She obtained her PhD in Communication\nStudies at the same institution and her research focuses on the relationship between journalism,\ndemocracy and audiences, online hate speech, news and civic literacy, news consumption and\naudience research, gender and journalism, and cultural journalism.\nJoep Hofhuis is an assistant professor at ESHCC’s Department of Media and Communication,\nwhere he specializes in intercultural and organizational communication. He was awarded a PhD in\nsocial and organizational psychology (University of Groningen, 2012), based on his research on\ncultural diversity in the workplace.
