{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Training a language model using Transformers\n",
        "\n",
        "Training a language model is a more advanced form of machine learning, therefore we need to install some libraries that are not default for Google Colab. We also clone our GitHub repository as usual."
      ],
      "metadata": {
        "id": "Rq_PLdJh_5hO"
      },
      "id": "Rq_PLdJh_5hO"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!git clone https://github.com/Joaoffg/AISocIMP23/"
      ],
      "metadata": {
        "id": "3jcgePv6iqDD"
      },
      "id": "3jcgePv6iqDD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing the libraries, we need to import them so we can use them in out code."
      ],
      "metadata": {
        "id": "9CkKgAaJAJ12"
      },
      "id": "9CkKgAaJAJ12"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3467bc6-9d41-4f89-b82c-23bfb82b603e",
      "metadata": {
        "id": "b3467bc6-9d41-4f89-b82c-23bfb82b603e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from transformers import LlamaTokenizer, LlamaConfig, LlamaModel, LlamaForCausalLM, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part loads all of the text data that you will use to train the language model."
      ],
      "metadata": {
        "id": "wzxNoqk-AP8F"
      },
      "id": "wzxNoqk-AP8F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa705ed2-e705-411e-98bb-bc96317e3a2d",
      "metadata": {
        "id": "aa705ed2-e705-411e-98bb-bc96317e3a2d"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"text\",\n",
        "                       data_dir=\"/content/AISocIMP23/Week 4/Texts\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part tokenizes the dataset, which means that it converts all of the words into numbers that can be processed by the neural network."
      ],
      "metadata": {
        "id": "ruXFeO68Ab-V"
      },
      "id": "ruXFeO68Ab-V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef7e878-1b11-4019-b4b2-9217e6d554a6",
      "metadata": {
        "id": "9ef7e878-1b11-4019-b4b2-9217e6d554a6"
      },
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained('/content/AISocIMP23/Week 4/Token')\n",
        "\n",
        "def chunk_examples(examples,chunk_lenght=128, min_chunk_lenght = 25):\n",
        "    chunks = []\n",
        "    for text in examples[\"text\"]:\n",
        "        tokenized = tokenizer(text,add_special_tokens=False)\n",
        "        if len(tokenized.input_ids) > min_chunk_lenght:\n",
        "            input_ids = [tokenizer.bos_token_id] + tokenized.input_ids + [tokenizer.eos_token_id]\n",
        "            attention_mask = [1] + tokenized.attention_mask + [1]\n",
        "            for i in range(0, len(tokenized.input_ids), chunk_lenght):\n",
        "                cunk_input_ids = input_ids[i:i + chunk_lenght]\n",
        "                cunk_att_mask = attention_mask[i:i + chunk_lenght]\n",
        "                cur_chunk_len = len(cunk_input_ids)\n",
        "\n",
        "                if  cur_chunk_len < chunk_lenght:\n",
        "                    cunk_input_ids = cunk_input_ids + [tokenizer.eos_token_id]*(chunk_lenght - cur_chunk_len)\n",
        "                    cunk_att_mask = cunk_att_mask + [0]*(chunk_lenght - cur_chunk_len)\n",
        "\n",
        "                chunks += [{\"input_ids\":torch.tensor(cunk_input_ids),\n",
        "                        \"attention_mask\": torch.tensor(cunk_att_mask),\n",
        "                        \"labels\": torch.tensor(cunk_input_ids)}\n",
        "                        #\"raw\":tokenizer.decode(cunk_input_ids)}\n",
        "\n",
        "                        ]\n",
        "\n",
        "\n",
        "    return {\"chunks\": chunks}\n",
        "\n",
        "\n",
        "chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define our model architecture, we are using a LlaMa based model for this exercise. You can change the complexity factor below to make the model more simple or more complex."
      ],
      "metadata": {
        "id": "o-nI9L8rAppV"
      },
      "id": "o-nI9L8rAppV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17a48d0-2377-4b62-92db-bd859847cb8d",
      "metadata": {
        "id": "a17a48d0-2377-4b62-92db-bd859847cb8d"
      },
      "outputs": [],
      "source": [
        "complexity_reduction=4\n",
        "\n",
        "config = LlamaConfig(\n",
        "    vocab_size = 32000,\n",
        "    hidden_size= int(2048/complexity_reduction),\n",
        "    intermediate_size = int(5120/complexity_reduction),\n",
        "    num_hidden_layers = int(16/complexity_reduction),\n",
        "    num_attention_heads = int(16/complexity_reduction),\n",
        "    max_position_embeddings = 2048 ,\n",
        "    rms_norm_eps = 1e-12\n",
        ")\n",
        "\n",
        "model = LlamaForCausalLM(config)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"LlaMa Model Size: {model_size/1000**2:.1f}M parameters\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you define the training arguments. You can ignore most of them, they are default values, but you may want to tweak the batch_sizes and the learning rate."
      ],
      "metadata": {
        "id": "SKqz5tSxA-0M"
      },
      "id": "SKqz5tSxA-0M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f739a3-8b16-407e-8439-2f22c40bf991",
      "metadata": {
        "id": "67f739a3-8b16-407e-8439-2f22c40bf991"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"erasmian-lm/medium\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    evaluation_strategy=\"no\",\n",
        "    eval_steps=5_000,\n",
        "    logging_steps=5_000,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=0.0001,\n",
        "    save_steps=5_000,\n",
        "    fp16=True,\n",
        "    save_strategy = \"epoch\", #save only latest model at end of epoch instead of 5k steps.\n",
        "    save_total_limit = 1 # save only latest 3 epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step trains the model, exciting!"
      ],
      "metadata": {
        "id": "aorFhYglBfJU"
      },
      "id": "aorFhYglBfJU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f62d16f-51c6-4d52-8c97-79198d877178",
      "metadata": {
        "id": "1f62d16f-51c6-4d52-8c97-79198d877178"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    train_dataset=chunked_dataset['train']['chunks']\n",
        "\n",
        ")\n",
        "\n",
        "model = torch.compile(model)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here you can test how the model is actually performing by generating some text. You can write the start of the text between \"\" in the input_text field."
      ],
      "metadata": {
        "id": "ABVa4YUcBkcD"
      },
      "id": "ABVa4YUcBkcD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0d26df-a476-4631-b08f-7c123c27fb4a",
      "metadata": {
        "id": "ef0d26df-a476-4631-b08f-7c123c27fb4a"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "input_text= \"Erasmus University is\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=1,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "inputs = inputs.to(\"cuda:0\")\n",
        "model = model.to(\"cuda:0\")\n",
        "outputs = model.generate(**inputs, num_beams=1, do_sample=True, max_length=128)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}