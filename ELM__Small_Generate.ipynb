{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MBq6DaFPmhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b017d0-cb5b-4bc3-edbe-fa6ba46ed992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ELM-small-0_1.zip\n",
            "   creating: erasmian-lm/\n",
            "  inflating: erasmian-lm/tokenizer.model  \n",
            "  inflating: erasmian-lm/tokenizer_config.json  \n",
            "  inflating: erasmian-lm/special_tokens_map.json  \n",
            "   creating: erasmian-lm/.ipynb_checkpoints/\n",
            "   creating: erasmian-lm/checkpoint-198349/\n",
            "  inflating: erasmian-lm/checkpoint-198349/config.json  \n",
            "  inflating: erasmian-lm/checkpoint-198349/generation_config.json  \n",
            "  inflating: erasmian-lm/checkpoint-198349/pytorch_model.bin  \n",
            "  inflating: erasmian-lm/checkpoint-198349/tokenizer_config.json  \n",
            "  inflating: erasmian-lm/checkpoint-198349/special_tokens_map.json  \n",
            "  inflating: erasmian-lm/checkpoint-198349/tokenizer.model  \n",
            "  inflating: erasmian-lm/checkpoint-198349/training_args.bin  \n",
            "  inflating: erasmian-lm/checkpoint-198349/optimizer.pt  \n",
            "  inflating: erasmian-lm/checkpoint-198349/scheduler.pt  \n",
            "  inflating: erasmian-lm/checkpoint-198349/trainer_state.json  \n",
            "  inflating: erasmian-lm/checkpoint-198349/rng_state.pth  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://surfdrive.surf.nl/files/index.php/s/9EQ0V9XlfbqZJpb/download\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(\"ELM-small-0_1.zip\", mode=\"wb\") as file:\n",
        "     file.write(response.content)\n",
        "\n",
        "!unzip /content/ELM-small-0_1.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "1XMEEgNWRWST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee08f4c-edab-47f1-9977-5467e3131f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "token_path=\"erasmian-lm\"\n",
        "model_path=\"erasmian-lm/checkpoint-198349\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(token_path)\n",
        "model = LlamaForCausalLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "RUQw2ZATQxIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d5d247-55b4-4092-aaa8-ff791e1e3602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    # temperature=1.3,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    # num_beams=2,\n",
        "    repetition_penalty=1,\n",
        "    do_sample=True, # allow 'beam sample': do_sample=True, num_beams > 1\n",
        "    num_return_sequences=1 # generate multiple sequences, takes longer..\n",
        ")"
      ],
      "metadata": {
        "id": "8VDIF-jsSC41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = tokenizer(\"Dit onderzoek gaat over\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**prompt, num_beams=1, do_sample=True, max_length=256)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0].replace('\\\\n','\\n'))"
      ],
      "metadata": {
        "id": "G7v0AwycSId5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a360981b-3745-4b6a-a079-7a4e9778b692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dit onderzoek gaat over het belang van het delen van kennis binnen een organisatie. Uit de\n",
            "resultaten blijkt dat kennisdeling in een organisatie belangrijk is voor het behouden van\n",
            "kennisdeling. De respondenten geven aan dat de rol van management bij kennisdeling is\n",
            "verbeteren. Er is veel onderzoek gedaan naar de invloed van management op kennisdeling\n",
            "en de rol van management in samenwerking. Het belang van kennisdeling kan ook van de\n",
            "kernbegrippen bekend gemaakt worden: de manier waarop men in elkaar steekt om gezamenlijk\n",
            "gezamenlijk kennis op te doen en het delen van vaardigheden/kennis. De kennis die men van\n",
            "kennis heeft, zal bijgedragen worden door kennisdelen en kennis ophalen via een\n",
            "kennisoverdracht en kennisoverdracht aan de klant. Dit is niet altijd de\n",
            "kennisoverdracht en kennisdeling via de kennisoverdracht. Bij het delen van\n",
            "kennis over de kennisoverdraaalspiegelden, delen kennis over\n",
            "de teamsamen op de kennisovergang in een shared governance wordt\n",
            "geken dat deze overkoragwisseling. In verschillende delen van kennis over gedeeld\n",
            "kennisdeling wordt kennisdeling tussen het delen van kennisdeling in teams en knowledge exchange ties in kennis overdracht tussen kennisdeling of kennis delen\n",
            "het kennisdeling\n"
          ]
        }
      ]
    }
  ]
}